{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model, Sequential\n",
    "from tensorflow.keras.layers import Input, Dense, Conv1D, MaxPooling1D, Flatten\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier\n",
    "\n",
    "# 결과 저장 경로 생성\n",
    "output_dir = \"../results/models/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# 데이터 로드\n",
    "df_cleaned = pd.read_csv(\"../data/processed/data_cleaned.csv\")\n",
    "\n",
    "# 데이터 분할 (특성(X)과 레이블(y) 분리)\n",
    "X = df_cleaned.drop(columns=['Label'])\n",
    "y = df_cleaned['Label']\n",
    "\n",
    "# 훈련/테스트 데이터 분할 (80% 훈련, 20% 테스트)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 데이터 정규화 (평균 0, 표준편차 1로 변환)\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate 544. MiB for an array with shape (2264474, 63) and data type float32",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 41\u001b[0m\n\u001b[0;32m     38\u001b[0m y_binary_train \u001b[38;5;241m=\u001b[39m (y_train \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)  \u001b[38;5;66;03m# 이진 분류 (0이면 0, 아니면 1)\u001b[39;00m\n\u001b[0;32m     39\u001b[0m y_binary_test \u001b[38;5;241m=\u001b[39m (y_test \u001b[38;5;241m!=\u001b[39m \u001b[38;5;241m0\u001b[39m)\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mint\u001b[39m)\n\u001b[1;32m---> 41\u001b[0m autoencoder_classifier\u001b[38;5;241m.\u001b[39mfit(\n\u001b[0;32m     42\u001b[0m     X_train_scaled, [X_train_scaled, y_binary_train],  \u001b[38;5;66;03m# 입력과 출력(원본 데이터와 분류 레이블)\u001b[39;00m\n\u001b[0;32m     43\u001b[0m     epochs\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m50\u001b[39m,\n\u001b[0;32m     44\u001b[0m     batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m128\u001b[39m,\n\u001b[0;32m     45\u001b[0m     validation_data\u001b[38;5;241m=\u001b[39m(X_test_scaled, [X_test_scaled, y_binary_test]),\n\u001b[0;32m     46\u001b[0m     callbacks\u001b[38;5;241m=\u001b[39m[early_stopping]\n\u001b[0;32m     47\u001b[0m )\n\u001b[0;32m     49\u001b[0m \u001b[38;5;66;03m# 모델 저장 (Keras 형식으로 저장)\u001b[39;00m\n\u001b[0;32m     50\u001b[0m autoencoder_classifier\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mautoencoder_classifier.keras\u001b[39m\u001b[38;5;124m\"\u001b[39m))\n",
      "File \u001b[1;32mc:\\Users\\dev\\anaconda3\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    119\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[0;32m    120\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[0;32m    121\u001b[0m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[1;32m--> 122\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    123\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[0;32m    124\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[1;32mc:\\Users\\dev\\anaconda3\\Lib\\site-packages\\optree\\ops.py:752\u001b[0m, in \u001b[0;36mtree_map\u001b[1;34m(func, tree, is_leaf, none_is_leaf, namespace, *rests)\u001b[0m\n\u001b[0;32m    750\u001b[0m leaves, treespec \u001b[38;5;241m=\u001b[39m _C\u001b[38;5;241m.\u001b[39mflatten(tree, is_leaf, none_is_leaf, namespace)\n\u001b[0;32m    751\u001b[0m flat_args \u001b[38;5;241m=\u001b[39m [leaves] \u001b[38;5;241m+\u001b[39m [treespec\u001b[38;5;241m.\u001b[39mflatten_up_to(r) \u001b[38;5;28;01mfor\u001b[39;00m r \u001b[38;5;129;01min\u001b[39;00m rests]\n\u001b[1;32m--> 752\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m treespec\u001b[38;5;241m.\u001b[39munflatten(\u001b[38;5;28mmap\u001b[39m(func, \u001b[38;5;241m*\u001b[39mflat_args))\n",
      "\u001b[1;31mMemoryError\u001b[0m: Unable to allocate 544. MiB for an array with shape (2264474, 63) and data type float32"
     ]
    }
   ],
   "source": [
    "# Autoencoder 커널 정의\n",
    "input_dim = X_train_scaled.shape[1]  # 입력 특성의 차원 (63)\n",
    "hidden_dim1 = 56  # 첫 번째 은닉층 차원\n",
    "encoding_dim = 49  # 차원 축소 목표 차원\n",
    "\n",
    "# 입력층 정의\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# 인코더 네트워크 (차원 축소)\n",
    "encoded = Dense(hidden_dim1, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# 디코더 네트워크 (복원 과정)\n",
    "decoded = Dense(hidden_dim1, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# 분류기 출력 (이진 분류기 추가)\n",
    "classification_output = Dense(1, activation='sigmoid')(encoded)\n",
    "\n",
    "# 1. Autoencoder + Binary Classifier 모델 정의\n",
    "autoencoder_classifier = Model(inputs=input_layer, outputs=[decoded, classification_output])\n",
    "autoencoder_classifier.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss=['mean_squared_error', 'binary_crossentropy'],  # Autoencoder와 분류 손실\n",
    "    loss_weights=[0.5, 0.5],  # 손실 가중치\n",
    "    metrics={'dense_4': 'accuracy'}  # 분류기 정확도 추적\n",
    ")\n",
    "\n",
    "# EarlyStopping 콜백 설정 (모델이 더 이상 개선되지 않으면 학습 중단)\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_dense_4_accuracy',  # 정확도 모니터링\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'  # 정확도를 최대화하려면 'max'\n",
    ")\n",
    "\n",
    "# Autoencoder + Binary Classifier 모델 학습\n",
    "y_binary_train = (y_train != 0).astype(int)  # 이진 분류 (0이면 0, 아니면 1)\n",
    "y_binary_test = (y_test != 0).astype(int)\n",
    "\n",
    "autoencoder_classifier.fit(\n",
    "    X_train_scaled, [X_train_scaled, y_binary_train],  # 입력과 출력(원본 데이터와 분류 레이블)\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test_scaled, [X_test_scaled, y_binary_test]),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 모델 저장 (Keras 형식으로 저장)\n",
    "autoencoder_classifier.save(os.path.join(output_dir, \"autoencoder_classifier.keras\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Autoencoder 차원 축소 (인코더만 추출)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# 차원 축소된 데이터 생성\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)\n",
    "\n",
    "# 차원 축소된 데이터 차원 확인\n",
    "print(\"Original shape:\", X_train_scaled.shape)  # 원래 차원: (샘플 수, 63)\n",
    "print(\"Reduced shape:\", X_train_encoded.shape)  # 축소된 차원: (샘플 수, 49)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Random Forest 이진 분류 모델 학습 (차원 축소된 데이터 사용)\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10, n_jobs=-1)\n",
    "rf.fit(X_train_encoded, y_binary_train)\n",
    "\n",
    "# Random Forest 모델 저장\n",
    "joblib.dump(rf, os.path.join(output_dir, \"autoencoder_random_forest.pkl\"))\n",
    "\n",
    "# 인코더 모델 저장 (HDF5 대신 Keras 형식으로 저장)\n",
    "encoder.save(os.path.join(output_dir, \"encoder.keras\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. UDBB 설정 및 Random Forest 기반 모델 정의\n",
    "\n",
    "# UDBB 설정 및 Random Forest 기반 모델 정의\n",
    "base_estimator = RandomForestClassifier(\n",
    "    n_estimators=100, random_state=42, n_jobs=-1\n",
    ")\n",
    "\n",
    "udbb_model = BalancedBaggingClassifier(\n",
    "    estimator=base_estimator,  # base_estimator -> estimator로 변경\n",
    "    sampling_strategy='auto',   # 자동으로 언더샘플링 비율 설정\n",
    "    replacement=False,          # 샘플링 시 중복 허용하지 않음\n",
    "    random_state=42,\n",
    "    n_estimators=100,           # 앙상블에 포함할 모델 개수\n",
    "    n_jobs=-1                   # 병렬 처리\n",
    ")\n",
    "\n",
    "# 모델 훈련 (차원 축소된 데이터를 사용)\n",
    "udbb_model.fit(X_train_encoded, y_train)\n",
    "\n",
    "# 모델 저장\n",
    "joblib.dump(udbb_model, os.path.join(output_dir, \"autoencoder_udbb.pkl\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. CNN 분류기 모델 구성 (Autoencoder로 차원 축소된 데이터 사용)\n",
    "cnn_classifier = Sequential([\n",
    "    Conv1D(filters=32, kernel_size=3, activation='relu', input_shape=(encoding_dim, 1)),  # 인코딩된 데이터 차원 사용\n",
    "    MaxPooling1D(pool_size=2),\n",
    "    Flatten(),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(15, activation='softmax')  # 다중 클래스 분류를 위한 소프트맥스 출력\n",
    "])\n",
    "\n",
    "# 모델 컴파일 (Adam optimizer와 sparse_categorical_crossentropy 손실 함수 사용)\n",
    "cnn_classifier.compile(\n",
    "    optimizer='adam',  # 최적화 기법\n",
    "    loss='sparse_categorical_crossentropy',  # 다중 클래스 분류 손실 함수\n",
    "    metrics=['accuracy']  # 평가 지표로 정확도 사용\n",
    ")\n",
    "\n",
    "# CNN 학습을 위해 입력 데이터 차원 변경 (CNN은 3D 입력을 요구)\n",
    "X_train_encoded_cnn = np.expand_dims(X_train_encoded, axis=-1)  # (샘플 수, 차원 수, 1)\n",
    "X_test_encoded_cnn = np.expand_dims(X_test_encoded, axis=-1)    # (샘플 수, 차원 수, 1)\n",
    "\n",
    "# CNN 모델 학습\n",
    "cnn_classifier.fit(\n",
    "    X_train_encoded_cnn, y_train,  # 타겟 데이터는 정수형(0~14)\n",
    "    epochs=30,\n",
    "    batch_size=64,\n",
    "    validation_data=(X_test_encoded_cnn, y_test)\n",
    ")\n",
    "\n",
    "# CNN 모델 저장\n",
    "cnn_classifier.save(os.path.join(output_dir, \"autoencoder_cnn.keras\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
