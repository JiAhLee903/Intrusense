{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import classification_report\n",
    "from tensorflow.keras.callbacks import EarlyStopping\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Input, Dense\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import joblib\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "# 데이터 로드\n",
    "df_cleaned = pd.read_csv(\"../data/processed/data_cleaned.csv\")\n",
    "\n",
    "# 데이터 분할\n",
    "X = df_cleaned.drop(columns=['Label'])\n",
    "y = df_cleaned['Label']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# 데이터 정규화\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# 정규화된 데이터를 저장\n",
    "joblib.dump(scaler, \"../results/models1/scaler.pkl\")\n",
    "\n",
    "# 결과 저장 경로 생성\n",
    "output_dir = \"../results/models1/\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m35s\u001b[0m 2ms/step - dense_3_loss: 0.7035 - dense_4_accuracy: 0.9669 - dense_4_loss: 0.0767 - loss: 0.3901 - val_dense_3_loss: 0.6911 - val_dense_4_accuracy: 0.9819 - val_dense_4_loss: 0.0458 - val_loss: 0.3685\n",
      "Epoch 2/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.7688 - dense_4_accuracy: 0.9795 - dense_4_loss: 0.0475 - loss: 0.4081 - val_dense_3_loss: 0.6907 - val_dense_4_accuracy: 0.9797 - val_dense_4_loss: 0.0446 - val_loss: 0.3677\n",
      "Epoch 3/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - dense_3_loss: 0.7015 - dense_4_accuracy: 0.9820 - dense_4_loss: 0.0414 - loss: 0.3714 - val_dense_3_loss: 0.6905 - val_dense_4_accuracy: 0.9850 - val_dense_4_loss: 0.0357 - val_loss: 0.3631\n",
      "Epoch 4/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.6816 - dense_4_accuracy: 0.9851 - dense_4_loss: 0.0353 - loss: 0.3584 - val_dense_3_loss: 0.6904 - val_dense_4_accuracy: 0.9862 - val_dense_4_loss: 0.0311 - val_loss: 0.3608\n",
      "Epoch 5/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.6895 - dense_4_accuracy: 0.9904 - dense_4_loss: 0.0263 - loss: 0.3579 - val_dense_3_loss: 0.6904 - val_dense_4_accuracy: 0.9931 - val_dense_4_loss: 0.0203 - val_loss: 0.3553\n",
      "Epoch 6/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.7123 - dense_4_accuracy: 0.9922 - dense_4_loss: 0.0219 - loss: 0.3671 - val_dense_3_loss: 0.6903 - val_dense_4_accuracy: 0.9946 - val_dense_4_loss: 0.0172 - val_loss: 0.3538\n",
      "Epoch 7/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.6760 - dense_4_accuracy: 0.9929 - dense_4_loss: 0.0197 - loss: 0.3479 - val_dense_3_loss: 0.6906 - val_dense_4_accuracy: 0.9892 - val_dense_4_loss: 0.0314 - val_loss: 0.3610\n",
      "Epoch 8/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.6689 - dense_4_accuracy: 0.9933 - dense_4_loss: 0.0187 - loss: 0.3438 - val_dense_3_loss: 0.6902 - val_dense_4_accuracy: 0.9922 - val_dense_4_loss: 0.0170 - val_loss: 0.3536\n",
      "Epoch 9/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.7252 - dense_4_accuracy: 0.9934 - dense_4_loss: 0.0184 - loss: 0.3718 - val_dense_3_loss: 0.6903 - val_dense_4_accuracy: 0.9949 - val_dense_4_loss: 0.0169 - val_loss: 0.3536\n",
      "Epoch 10/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.7611 - dense_4_accuracy: 0.9939 - dense_4_loss: 0.0171 - loss: 0.3891 - val_dense_3_loss: 0.6876 - val_dense_4_accuracy: 0.9951 - val_dense_4_loss: 0.0158 - val_loss: 0.3517\n",
      "Epoch 11/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.7040 - dense_4_accuracy: 0.9942 - dense_4_loss: 0.0165 - loss: 0.3602 - val_dense_3_loss: 0.6877 - val_dense_4_accuracy: 0.9943 - val_dense_4_loss: 0.0155 - val_loss: 0.3516\n",
      "Epoch 12/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m32s\u001b[0m 2ms/step - dense_3_loss: 0.7085 - dense_4_accuracy: 0.9945 - dense_4_loss: 0.0160 - loss: 0.3623 - val_dense_3_loss: 0.6876 - val_dense_4_accuracy: 0.9962 - val_dense_4_loss: 0.0136 - val_loss: 0.3506\n",
      "Epoch 13/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.7259 - dense_4_accuracy: 0.9947 - dense_4_loss: 0.0154 - loss: 0.3707 - val_dense_3_loss: 0.6875 - val_dense_4_accuracy: 0.9957 - val_dense_4_loss: 0.0131 - val_loss: 0.3503\n",
      "Epoch 14/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.7072 - dense_4_accuracy: 0.9947 - dense_4_loss: 0.0150 - loss: 0.3611 - val_dense_3_loss: 0.6876 - val_dense_4_accuracy: 0.9952 - val_dense_4_loss: 0.0130 - val_loss: 0.3503\n",
      "Epoch 15/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.7140 - dense_4_accuracy: 0.9950 - dense_4_loss: 0.0145 - loss: 0.3642 - val_dense_3_loss: 0.6870 - val_dense_4_accuracy: 0.9959 - val_dense_4_loss: 0.0123 - val_loss: 0.3496\n",
      "Epoch 16/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.6872 - dense_4_accuracy: 0.9950 - dense_4_loss: 0.0143 - loss: 0.3508 - val_dense_3_loss: 0.6870 - val_dense_4_accuracy: 0.9959 - val_dense_4_loss: 0.0122 - val_loss: 0.3496\n",
      "Epoch 17/50\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m31s\u001b[0m 2ms/step - dense_3_loss: 0.6861 - dense_4_accuracy: 0.9950 - dense_4_loss: 0.0141 - loss: 0.3501 - val_dense_3_loss: 0.6870 - val_dense_4_accuracy: 0.9955 - val_dense_4_loss: 0.0136 - val_loss: 0.3503\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 723us/step\n",
      "Classification Report for Autoencoder + Classifier:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    454590\n",
      "           1       0.99      0.99      0.99    111529\n",
      "\n",
      "    accuracy                           1.00    566119\n",
      "   macro avg       0.99      0.99      0.99    566119\n",
      "weighted avg       1.00      1.00      1.00    566119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 1. Autoencoder + Classifier 모델 정의\n",
    "input_dim = X_train_scaled.shape[1]  # 입력 차원 (63)\n",
    "hidden_dim1 = 56  # 중간 은닉층\n",
    "encoding_dim = 49  # 차원 축소 목표\n",
    "\n",
    "# 입력층\n",
    "input_layer = Input(shape=(input_dim,))\n",
    "\n",
    "# 인코더\n",
    "encoded = Dense(hidden_dim1, activation='relu')(input_layer)\n",
    "encoded = Dense(encoding_dim, activation='relu')(encoded)\n",
    "\n",
    "# 디코더 (Autoencoder의 복원 부분)\n",
    "decoded = Dense(hidden_dim1, activation='relu')(encoded)\n",
    "decoded = Dense(input_dim, activation='sigmoid')(decoded)\n",
    "\n",
    "# 분류기 (분류 태스크를 위한 출력 레이어 추가)\n",
    "classification_output = Dense(1, activation='sigmoid')(encoded)\n",
    "\n",
    "# Autoencoder + Classifier 모델 정의\n",
    "autoencoder_classifier = Model(inputs=input_layer, outputs=[decoded, classification_output])\n",
    "autoencoder_classifier.compile(\n",
    "    optimizer=Adam(),\n",
    "    loss=['mean_squared_error', 'binary_crossentropy'],  # Autoencoder 복원 + 분류 손실\n",
    "    loss_weights=[0.5, 0.5],  # 손실 가중치\n",
    "    metrics={'dense_4': 'accuracy'}  # 분류기의 출력에 해당하는 정확도 추적 (dense_4는 classification_output 레이어)\n",
    ")\n",
    "\n",
    "# EarlyStopping 콜백\n",
    "early_stopping = EarlyStopping(\n",
    "    monitor='val_dense_4_accuracy',  # classification_output의 정확도를 모니터링\n",
    "    patience=5,\n",
    "    restore_best_weights=True,\n",
    "    mode='max'  # 정확도는 최대화해야 하므로 'max'로 설정\n",
    ")\n",
    "# 2. Autoencoder + Classifier 학습\n",
    "y_binary_train = (y_train != 0).astype(int)\n",
    "y_binary_test = (y_test != 0).astype(int)\n",
    "\n",
    "autoencoder_classifier.fit(\n",
    "    X_train_scaled, [X_train_scaled, y_binary_train],  # 입력 데이터와 분류 레이블\n",
    "    epochs=50,\n",
    "    batch_size=128,\n",
    "    validation_data=(X_test_scaled, [X_test_scaled, y_binary_test]),\n",
    "    callbacks=[early_stopping]\n",
    ")\n",
    "\n",
    "# 3. 평가: Autoencoder + Classifier\n",
    "_, y_pred_binary = autoencoder_classifier.predict(X_test_scaled)\n",
    "\n",
    "y_pred_binary = (y_pred_binary > 0.5).astype(int)\n",
    "print(\"Classification Report for Autoencoder + Classifier:\")\n",
    "print(classification_report(y_binary_test, y_pred_binary))\n",
    "\n",
    "# 모델 저장\n",
    "autoencoder_classifier.save(os.path.join(output_dir, \"autoencoder_classifier.h5\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m70765/70765\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m43s\u001b[0m 611us/step\n",
      "\u001b[1m17692/17692\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 600us/step\n",
      "Original shape: (2264474, 63)\n",
      "Reduced shape: (2264474, 49)\n",
      "Classification Report for Random Forest:\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00    454590\n",
      "           1       1.00      0.99      0.99    111529\n",
      "\n",
      "    accuracy                           1.00    566119\n",
      "   macro avg       1.00      0.99      1.00    566119\n",
      "weighted avg       1.00      1.00      1.00    566119\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 4. Autoencoder 차원 축소 (인코더만 추출하여 사용)\n",
    "encoder = Model(inputs=input_layer, outputs=encoded)\n",
    "\n",
    "# 차원 축소된 데이터\n",
    "X_train_encoded = encoder.predict(X_train_scaled)\n",
    "X_test_encoded = encoder.predict(X_test_scaled)\n",
    "\n",
    "# 축소된 차원 확인\n",
    "print(\"Original shape:\", X_train_scaled.shape)  # 원래 차원: (샘플 수, 63)\n",
    "print(\"Reduced shape:\", X_train_encoded.shape)  # 축소 차원: (샘플 수, 49)\n",
    "\n",
    "# 5. Random Forest 모델 학습\n",
    "rf = RandomForestClassifier(random_state=42, n_estimators=100, max_depth=10)\n",
    "rf.fit(X_train_encoded, y_binary_train)\n",
    "\n",
    "# Random Forest 모델 저장\n",
    "joblib.dump(rf, os.path.join(output_dir, \"random_forest.pkl\"))\n",
    "\n",
    "# 6. 평가: Random Forest\n",
    "y_pred_rf = rf.predict(X_test_encoded)\n",
    "print(\"Classification Report for Random Forest:\")\n",
    "print(classification_report(y_binary_test, y_pred_rf))\n",
    "\n",
    "# 7. 인코더 모델 저장\n",
    "encoder.save(os.path.join(output_dir, \"encoder.h5\"))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
